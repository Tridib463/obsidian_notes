- - -
## Intro ->

LLMs are an instance of Foundation Model.
![[Pasted image 20240412223138.png|240]]
Foundation Models are pre-Trained on large amounts of unlabeled and self-supervised data such as Texts. Self-supervised learning does not require manual labelling of data.

1. LLMs have much larger number of models parameters. A Parameter is a value a model can change independently as it learns. The more parameters there are , the more complex the model is.
2. One unique feature of LLMs is 'Zero-shot Learning'. It is the capability of a ML Model to complete a task it was not explicitly trained to do.

![[Pasted image 20240412224431.png|700]]

- - -
## How do they work?
### LLM = Data + Architecture + Training
Architecture -> Artificial Neural Network and for GPT that is a Transformer.
- Transformer model enables the model to handle sequences of data like sequences of words.
- Transformers are designed to understand the Context of each word in a Sentence by considering it in relation to every word. This allows the model to build a comprehensive understanding of the sentence.
Training ->
 - During training, the model learns to predict the next word in a sentence.
 - First it starts off with a Random Guess ,  but with each iteration the model adjusts its internal parameters to reduce the difference between its predictions and the Actual outcomes. kind of like training a Baby.
The model can be fine-Tuned on a smaller , more specific Data-set.
- The model also need to undergo Human training called Reinforcement Learning with Human Feedback (RLHF). kind of like training a Dog.

When training is done , the Model is Mostly frozen other than some Fine-Tuning that can happen later. So GPT does not learn continuously , it is already Pretrained. 
- - -
## 3 Levels of using LLMs
1. Prompt Engineering
2. Model Fine-Tuning
3. Build your own LLM

### Prompt Engineering->
Using an LLM out-of-the-box i.e. Not changing any Model Parameters
![[Pasted image 20240413081038.png|600]]
Open Source soln. is the Hugging Face "Transformers Library" which gives access to Open source LLMs. We can run this models locally on our machine.

### Fine-Tuning->
Adjusting at least 1 (internal) model parameter (but not all) for a particular task.
![[Pasted image 20240413081720.png|450]]
Step 1 -> Self-Supervised Learning
Step 2 -> Supervised and Reinforcement Learning to tweak the model  parameters according to specific use case.
eg- ChatGPT is actually a Fine-Tuned Model ... Not the Raw LLM.

